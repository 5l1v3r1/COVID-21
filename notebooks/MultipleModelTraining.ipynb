{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T13:19:35.884995Z",
     "start_time": "2020-03-23T13:19:34.222942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.2.0\n",
      "Torchvision Version:  0.4.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import torch.optim as optim\n",
    "import torchvision.models as m\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from os.path import dirname, abspath\n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "random.seed = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T13:19:35.927533Z",
     "start_time": "2020-03-23T13:19:35.924666Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T13:19:35.947326Z",
     "start_time": "2020-03-23T13:19:35.944301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T13:19:36.076690Z",
     "start_time": "2020-03-23T13:19:36.072925Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_model(num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "    \n",
    "    model_ft = m.resnet50(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    input_size = 224\n",
    "    \n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T13:19:36.649278Z",
     "start_time": "2020-03-23T13:19:36.646115Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T13:19:37.070446Z",
     "start_time": "2020-03-23T13:19:37.060775Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet50\n",
    "        \"\"\"\n",
    "        model_ft = m.resnet50(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = m.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = m.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = m.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = m.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T13:19:38.122366Z",
     "start_time": "2020-03-23T13:19:38.110882Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model_name, model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'{model_name} Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T13:39:48.843419Z",
     "start_time": "2020-03-23T13:25:29.813424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "resnet Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.7127 Acc: 0.5500\n",
      "val Loss: 0.7255 Acc: 0.3636\n",
      "\n",
      "resnet Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.7192 Acc: 0.4750\n",
      "val Loss: 0.6488 Acc: 0.5909\n",
      "\n",
      "resnet Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.6688 Acc: 0.5000\n",
      "val Loss: 0.5644 Acc: 0.7727\n",
      "\n",
      "resnet Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.6073 Acc: 0.6250\n",
      "val Loss: 0.7023 Acc: 0.5000\n",
      "\n",
      "resnet Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.5877 Acc: 0.7000\n",
      "val Loss: 0.5385 Acc: 0.6818\n",
      "\n",
      "resnet Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.5052 Acc: 0.7000\n",
      "val Loss: 0.5736 Acc: 0.5909\n",
      "\n",
      "resnet Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.4455 Acc: 0.7750\n",
      "val Loss: 0.3771 Acc: 0.9545\n",
      "\n",
      "resnet Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.3851 Acc: 0.8500\n",
      "val Loss: 0.3332 Acc: 0.9545\n",
      "\n",
      "resnet Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.3450 Acc: 0.8750\n",
      "val Loss: 0.3551 Acc: 0.8182\n",
      "\n",
      "resnet Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.3255 Acc: 0.9250\n",
      "val Loss: 0.2622 Acc: 0.9545\n",
      "\n",
      "resnet Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.2463 Acc: 0.9250\n",
      "val Loss: 0.2106 Acc: 0.9545\n",
      "\n",
      "resnet Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.2446 Acc: 0.9500\n",
      "val Loss: 0.1817 Acc: 1.0000\n",
      "\n",
      "resnet Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.3772 Acc: 0.7750\n",
      "val Loss: 0.1689 Acc: 1.0000\n",
      "\n",
      "resnet Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.2394 Acc: 0.9750\n",
      "val Loss: 0.1641 Acc: 0.9545\n",
      "\n",
      "resnet Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.2281 Acc: 0.9750\n",
      "val Loss: 0.1622 Acc: 0.9545\n",
      "\n",
      "Training complete in 0m 16s\n",
      "Best val Acc: 1.000000\n",
      "Saving finetuned model resnet...\n",
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "alexnet Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.7267 Acc: 0.5750\n",
      "val Loss: 0.8729 Acc: 0.5455\n",
      "\n",
      "alexnet Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.4897 Acc: 0.7750\n",
      "val Loss: 0.6627 Acc: 0.6364\n",
      "\n",
      "alexnet Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.4202 Acc: 0.8750\n",
      "val Loss: 0.3879 Acc: 0.8636\n",
      "\n",
      "alexnet Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.5024 Acc: 0.7750\n",
      "val Loss: 0.3736 Acc: 0.8182\n",
      "\n",
      "alexnet Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.2232 Acc: 0.9000\n",
      "val Loss: 0.4052 Acc: 0.9091\n",
      "\n",
      "alexnet Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.4680 Acc: 0.8000\n",
      "val Loss: 0.7637 Acc: 0.7727\n",
      "\n",
      "alexnet Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.3079 Acc: 0.8500\n",
      "val Loss: 0.4469 Acc: 0.8182\n",
      "\n",
      "alexnet Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.1016 Acc: 0.9750\n",
      "val Loss: 0.3210 Acc: 0.9091\n",
      "\n",
      "alexnet Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.4455 Acc: 0.8250\n",
      "val Loss: 0.3174 Acc: 0.8182\n",
      "\n",
      "alexnet Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.1281 Acc: 0.9500\n",
      "val Loss: 0.5055 Acc: 0.8636\n",
      "\n",
      "alexnet Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.2134 Acc: 0.9250\n",
      "val Loss: 0.2878 Acc: 0.8636\n",
      "\n",
      "alexnet Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.2761 Acc: 0.9250\n",
      "val Loss: 0.2955 Acc: 0.8182\n",
      "\n",
      "alexnet Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.4121 Acc: 0.8750\n",
      "val Loss: 0.2462 Acc: 0.9091\n",
      "\n",
      "alexnet Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.2375 Acc: 0.9500\n",
      "val Loss: 0.3753 Acc: 0.9091\n",
      "\n",
      "alexnet Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.5997 Acc: 0.7750\n",
      "val Loss: 0.5568 Acc: 0.8636\n",
      "\n",
      "Training complete in 0m 15s\n",
      "Best val Acc: 0.909091\n",
      "Saving finetuned model alexnet...\n",
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "vgg Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.6909 Acc: 0.6250\n",
      "val Loss: 0.6640 Acc: 0.5000\n",
      "\n",
      "vgg Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.5861 Acc: 0.6750\n",
      "val Loss: 0.5056 Acc: 0.9091\n",
      "\n",
      "vgg Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.4197 Acc: 0.9500\n",
      "val Loss: 0.3803 Acc: 0.9545\n",
      "\n",
      "vgg Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.2946 Acc: 0.9250\n",
      "val Loss: 0.2969 Acc: 0.9545\n",
      "\n",
      "vgg Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.3383 Acc: 0.8500\n",
      "val Loss: 0.2579 Acc: 0.9545\n",
      "\n",
      "vgg Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.2451 Acc: 0.9750\n",
      "val Loss: 0.2073 Acc: 0.9545\n",
      "\n",
      "vgg Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.2462 Acc: 0.9000\n",
      "val Loss: 0.1852 Acc: 0.9545\n",
      "\n",
      "vgg Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.3046 Acc: 0.8750\n",
      "val Loss: 0.1922 Acc: 0.9091\n",
      "\n",
      "vgg Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.1644 Acc: 1.0000\n",
      "val Loss: 0.1903 Acc: 0.9091\n",
      "\n",
      "vgg Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.2237 Acc: 0.9500\n",
      "val Loss: 0.1269 Acc: 0.9545\n",
      "\n",
      "vgg Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.1427 Acc: 0.9750\n",
      "val Loss: 0.1049 Acc: 0.9545\n",
      "\n",
      "vgg Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.2797 Acc: 0.9250\n",
      "val Loss: 0.0863 Acc: 0.9545\n",
      "\n",
      "vgg Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.2385 Acc: 0.8750\n",
      "val Loss: 0.0731 Acc: 1.0000\n",
      "\n",
      "vgg Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.1870 Acc: 0.9500\n",
      "val Loss: 0.0686 Acc: 1.0000\n",
      "\n",
      "vgg Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.1216 Acc: 0.9500\n",
      "val Loss: 0.0700 Acc: 1.0000\n",
      "\n",
      "Training complete in 0m 16s\n",
      "Best val Acc: 1.000000\n",
      "Saving finetuned model vgg...\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "squeezenet Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.9130 Acc: 0.2750\n",
      "val Loss: 0.7066 Acc: 0.5455\n",
      "\n",
      "squeezenet Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.7120 Acc: 0.4750\n",
      "val Loss: 0.8125 Acc: 0.5000\n",
      "\n",
      "squeezenet Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.6453 Acc: 0.5500\n",
      "val Loss: 0.7963 Acc: 0.4545\n",
      "\n",
      "squeezenet Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.6401 Acc: 0.7500\n",
      "val Loss: 0.7454 Acc: 0.5000\n",
      "\n",
      "squeezenet Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.5976 Acc: 0.7500\n",
      "val Loss: 0.6803 Acc: 0.6818\n",
      "\n",
      "squeezenet Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.5283 Acc: 0.8000\n",
      "val Loss: 0.6098 Acc: 0.7727\n",
      "\n",
      "squeezenet Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.4762 Acc: 0.8000\n",
      "val Loss: 0.5810 Acc: 0.6818\n",
      "\n",
      "squeezenet Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.4994 Acc: 0.7500\n",
      "val Loss: 0.5744 Acc: 0.6818\n",
      "\n",
      "squeezenet Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.4055 Acc: 0.8500\n",
      "val Loss: 0.4512 Acc: 0.9091\n",
      "\n",
      "squeezenet Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.3784 Acc: 0.8750\n",
      "val Loss: 0.3650 Acc: 0.9091\n",
      "\n",
      "squeezenet Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.3883 Acc: 0.9000\n",
      "val Loss: 0.3307 Acc: 0.8636\n",
      "\n",
      "squeezenet Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.2700 Acc: 0.9000\n",
      "val Loss: 0.2868 Acc: 0.9091\n",
      "\n",
      "squeezenet Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.2421 Acc: 0.9000\n",
      "val Loss: 0.2384 Acc: 0.9545\n",
      "\n",
      "squeezenet Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.3207 Acc: 0.8750\n",
      "val Loss: 0.2230 Acc: 0.9545\n",
      "\n",
      "squeezenet Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.2148 Acc: 0.9500\n",
      "val Loss: 0.3989 Acc: 0.6818\n",
      "\n",
      "Training complete in 0m 14s\n",
      "Best val Acc: 0.954545\n",
      "Saving finetuned model squeezenet...\n",
      "Params to learn:\n",
      "\t classifier.weight\n",
      "\t classifier.bias\n",
      "densenet Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.7562 Acc: 0.5250\n",
      "val Loss: 0.6892 Acc: 0.5909\n",
      "\n",
      "densenet Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.6636 Acc: 0.6000\n",
      "val Loss: 0.5962 Acc: 0.6818\n",
      "\n",
      "densenet Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.5665 Acc: 0.8000\n",
      "val Loss: 0.5752 Acc: 0.6818\n",
      "\n",
      "densenet Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.3881 Acc: 0.9250\n",
      "val Loss: 0.4069 Acc: 0.8636\n",
      "\n",
      "densenet Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.3369 Acc: 0.9000\n",
      "val Loss: 0.3295 Acc: 0.9091\n",
      "\n",
      "densenet Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.3690 Acc: 0.8250\n",
      "val Loss: 0.3245 Acc: 0.8636\n",
      "\n",
      "densenet Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.3475 Acc: 0.8750\n",
      "val Loss: 0.2547 Acc: 0.9545\n",
      "\n",
      "densenet Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.2711 Acc: 0.9250\n",
      "val Loss: 0.1952 Acc: 0.9545\n",
      "\n",
      "densenet Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.1658 Acc: 0.9750\n",
      "val Loss: 0.2185 Acc: 0.9545\n",
      "\n",
      "densenet Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.1763 Acc: 1.0000\n",
      "val Loss: 0.2558 Acc: 0.9545\n",
      "\n",
      "densenet Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.2336 Acc: 0.9250\n",
      "val Loss: 0.1890 Acc: 0.9545\n",
      "\n",
      "densenet Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.2422 Acc: 0.9750\n",
      "val Loss: 0.1735 Acc: 0.9545\n",
      "\n",
      "densenet Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.2018 Acc: 0.9500\n",
      "val Loss: 0.2407 Acc: 0.9545\n",
      "\n",
      "densenet Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.3510 Acc: 0.9000\n",
      "val Loss: 0.2320 Acc: 0.9545\n",
      "\n",
      "densenet Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.3053 Acc: 0.8750\n",
      "val Loss: 0.2079 Acc: 0.9545\n",
      "\n",
      "Training complete in 0m 16s\n",
      "Best val Acc: 0.954545\n",
      "Saving finetuned model densenet...\n"
     ]
    }
   ],
   "source": [
    "dts = ['covid19_vs_normal/xray_dataset','ct_vs_xray','medical_vs_nonmedical']\n",
    "models = ['resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet']\n",
    "input_size = 224\n",
    "dataset = dts[0]\n",
    "epochs = 14\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "data_dir = f'{DIR}/datasets/{dataset}'\n",
    "\n",
    "data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    }\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=0) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Start Training Multiple Models\n",
    "for model_name in models:\n",
    "    # Initialize the model for this run\n",
    "    model_ft = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "    # Data augmentation and normalization for training\n",
    "    # Just normalization for validation\n",
    "    # Send the model to GPU\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    # Gather the parameters to be optimized/updated in this run. If we are\n",
    "    #  finetuning we will be updating all parameters. However, if we are\n",
    "    #  doing feature extract method, we will only update the parameters\n",
    "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "    #  is True.\n",
    "    params_to_update = model_ft.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate\n",
    "    model_ft, hist = train_model(model_name, model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "    torch.save(model_ft.state_dict(), f'{DIR}/notebooks/checkpoints/chk_{model_name}_finetuned_epoch_{num_epochs}.pt')\n",
    "    print(f'Saving finetuned model {model_name}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
